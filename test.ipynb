{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T03:43:16.425258Z",
     "start_time": "2024-08-06T03:43:14.998619Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "# from model.archi_Former import MM_Former"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "MRM_dict=torch.load('C:/Users/Alex/Downloads/MRM.pth',map_location='cpu')\n",
    "# ALB_dict=torch.load('C:/Users/Alex/Downloads/ALBEF.pth',map_location='cpu')\n",
    "# model = MM_Former(patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12,\n",
    "#         decoder_embed_dim=768, decoder_depth=4, decoder_num_heads=6,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), norm_pix_loss=True)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# \n",
    "# \n",
    "# # For a model\n",
    "# model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee03371a3d7e9cf1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# MRM\n",
    "for key,v in MRM_dict['model'].items():\n",
    "    print(key)\n",
    "    print(MRM_dict['model'][key].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e1d103aff8868e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# MM\n",
    "state_dict = model.state_dict()\n",
    "# 打印所有键\n",
    "for key in state_dict.keys():\n",
    "    print(key)\n",
    "    print(state_dict[key].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "589918274617351d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ALBEF\n",
    "for key in ALB_dict['model'].keys():\n",
    "    print(key)\n",
    "    print(ALB_dict['model'][key].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69e748b1b26b8bb1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "updated_dict = {}\n",
    "# load from MRM\n",
    "for key in state_dict.keys():\n",
    "    if key.startswith('decoder_'):\n",
    "        updated_dict[key]=MRM_dict['model'][key]\n",
    "    # elif key.startswith('bert_encoder.bert.pooler'):\n",
    "    #     new_key= key.replace('bert_encoder.bert.pooler', 'bert_encoder.model.bert.pooler')\n",
    "    #     updated_dict[key]=MRM_dict['model'][new_key]\n",
    "    # elif key.startswith('bert_encoder.cls.predictions'):\n",
    "    #     new_key= key.replace('bert_encoder.cls.predictions', 'bert_encoder.model.cls.predictions')\n",
    "    #     updated_dict[key]=MRM_dict['model'][new_key]\n",
    "        \n",
    "\n",
    "# load from ALBEF\n",
    "for key in state_dict.keys():\n",
    "    # 修改键名以匹配模型中的对应层\n",
    "    if key.startswith('vision_proj') or key.startswith('text_proj'):\n",
    "        updated_dict[key]=ALB_dict['model'][key]\n",
    "    if key.startswith('blocks'): \n",
    "        new_key = key.replace('blocks','visual_encoder.blocks')\n",
    "        updated_dict[key] = ALB_dict['model'][new_key]\n",
    "    elif key.startswith('norm'):\n",
    "        new_key = key.replace('norm','visual_encoder.norm', )\n",
    "        updated_dict[key] = ALB_dict['model'][new_key]\n",
    "    # elif key.startswith('bert_encoder.bert.embeddings'):\n",
    "    #     new_key = key.replace('bert_encoder.bert.embeddings', 'text_encoder.bert.embeddings')\n",
    "    elif key.startswith('bert_encoder.bert.encoder.layer'):\n",
    "        layer_num = int(key.split('.')[4])\n",
    "        new_key = key.replace(f'bert_encoder.bert.encoder.layer.{layer_num}',f'text_encoder.bert.encoder.layer.{layer_num}')\n",
    "        updated_dict[key] = ALB_dict['model'][new_key]\n",
    "    elif key.startswith('fusion_encoder.bert.encoder.layer'):\n",
    "        layer_num = int(key.split('.')[4])\n",
    "        new_key = key.replace(f'fusion_encoder.bert.encoder.layer.{layer_num}', f'text_encoder.bert.encoder.layer.{layer_num+6}')\n",
    "        updated_dict[key] = ALB_dict['model'][new_key]\n",
    "    # load bert.pooler and cls:\n",
    "    elif key.startswith('fusion_encoder.cls.predictions'):\n",
    "        new_key = key.replace('fusion_encoder.cls.predictions', 'text_encoder.cls.predictions')\n",
    "        updated_dict[key] = ALB_dict['model'][new_key]\n",
    "    elif key.startswith('bert_encoder.bert.embeddings'):\n",
    "        new_key= key.replace('bert_encoder.bert.embeddings', 'text_encoder.bert.embeddings')\n",
    "        updated_dict[key]=ALB_dict['model'][new_key]\n",
    "    elif key.startswith('temp'):\n",
    "        updated_dict['temp']=ALB_dict['model']['temp']\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a022272beaf0e6ce",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 将 state_dict 加载到模型中，strict 设置为 False 允许不匹配的键\n",
    "load_result = model.load_state_dict(updated_dict, strict=False)\n",
    "\n",
    "# load_state_dict 返回一个字典，包含 'missing_keys' 和 'unexpected_keys'\n",
    "missing_keys = load_result.missing_keys\n",
    "unexpected_keys = load_result.unexpected_keys\n",
    "print(f\"Missing keys: {missing_keys}\")\n",
    "print(f\"Unexpected keys: {unexpected_keys}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69a2d47684f874d4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "save={}\n",
    "save['model']=model.state_dict()\n",
    "torch.save(save,'C:/Users/Alex/Downloads/MM.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d3189c3ff49f1a8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from classification.model_ViT import MyViTClassifier\n",
    "model=MyViTClassifier()\n",
    "# print model state_dict\n",
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "308385baec78bda1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Paths to your directories (adjust as necessary)\n",
    "base_dir = '/home/data/Jingkai/alex/mimic/files'\n",
    "# Path for the output CSV file\n",
    "output_csv_path = 'training.csv'\n",
    "def find_final_report(content):\n",
    "    # Search for the start of the final report\n",
    "    start_index = content.find('FINAL REPORT')\n",
    "    if start_index != -1:\n",
    "        # Return the content from 'FINAL REPORT' onwards\n",
    "        return content[start_index:]\n",
    "    else:\n",
    "        # If 'FINAL REPORT' not found, return None or empty string\n",
    "        return None\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['image_path', 'report_content'])\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.jpg'):\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # Change the extension from .jpg to .txt to find the corresponding report\n",
    "                report_filename = os.path.splitext(image_path)[0] + '.txt'\n",
    "                report_path = os.path.join(root, report_filename)\n",
    "                \n",
    "                # Read the report content\n",
    "                try:\n",
    "                    with open(report_path, 'r', encoding='utf-8') as report_file:       \n",
    "                        report_content = report_file.read()\n",
    "                        # Find and extract 'FINAL REPORT' content\n",
    "                        final_report_content = find_final_report(report_content)\n",
    "                        if final_report_content:\n",
    "                            # Replace newlines with spaces\n",
    "                            final_report_content = final_report_content.replace('\\n', ' ').strip()\n",
    "                            # Write the image path and processed report content to the CSV\n",
    "                            writer.writerow([image_path, final_report_content])\n",
    "                        else:\n",
    "                            print(f\"'FINAL REPORT' not found in: {report_filename}\")\n",
    "                        \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Report file not found for image: {file_name}\")\n",
    "\n",
    "print(\"CSV file has been created.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bd5f51e197988ea",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from model.archi_Former import MM_Former\n",
    "import torch\n",
    "from model.archi import MM\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "fake_images = torch.rand(2, 3, 448, 448)  # 模拟图像数据\n",
    "text= ['sadflj123','231']\n",
    "batch= {\n",
    "    'image1': fake_images,\n",
    "    'text': text\n",
    "\n",
    "}\n",
    "model = MM_Former(patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=768, decoder_depth=4, decoder_num_heads=6,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), norm_pix_loss=True,local_contrastive_loss=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# For a model\n",
    "model = model.to(device)\n",
    "\n",
    "# 将模型转换为评估模式（这对于某些模块如Dropout和BatchNorm很重要）\n",
    "model.eval()\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():  # 不计算梯度，减少内存/计算需求\n",
    "    output = model(batch)\n",
    "\n",
    "# 检查输出\n",
    "print(output[1].shape)\n",
    "print(output[2].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0bbb00f9bb9fc4f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/submodule/bert/bert-base-uncased')\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
    "# \n",
    "# # 示例文本\n",
    "# text = \"Here is some example text to tokenize.\"\n",
    "# \n",
    "# # 对文本进行编码\n",
    "encoded_input = tokenizer(['23[SEP]'], truncation=True, padding=True, max_length=1, return_tensors=\"pt\")\n",
    "# # print(\"Encoded Token IDs:\", encoded_input)\n",
    "# e=encoded_input.input_ids\n",
    "# # 将编码后的token IDs转换回字符串\n",
    "# tokens = tokenizer.convert_ids_to_tokens(e[0])\n",
    "# print(\"Tokens:\", tokens)\n",
    "# \n",
    "# for i,j in zip(tokens,range(len(tokens))):\n",
    "#     # make i the string\n",
    "#     print(i,j)\n",
    "print(tokenizer.bos_token_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa95e9967a793fe0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_root='C:/Users/Alex/Desktop'\n",
    "csv_path=os.path.join(data_root,'training_mv.csv')\n",
    "df = pd.read_csv(csv_path, sep=',')\n",
    "i,vt,re=df[\"image_path\"], df['view_type'], df[\"report_content\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2798e397ac3d8eb1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "blip=torch.load('/home/data/Jingkai/alex/weight/blip2_pretrained.pth',map_location='cpu')\n",
    "# blip1=torch.load('C:/Users/Alex/Downloads/eva_vit_g.pth',map_location='cpu')\n",
    "for key in blip['model'].keys():\n",
    "    print(key)\n",
    "    print(blip['model'][key].shape)\n",
    "# blip1=blip1['model']\n",
    "# print('##################')\n",
    "# for key in blip1.keys():\n",
    "#     print(key)\n",
    "#     print(blip1[key].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58e415177e4ae80",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "result=np.load(r'C:\\Users\\Alex\\Desktop\\result.npy',allow_pickle=True)\n",
    "i2t=result.item().get('i2t')\n",
    "t2i=result.item().get('t2i')\n",
    "# get the variance of each line\n",
    "i2t_var= np.var(i2t, axis=1)\n",
    "t2i_var= np.var(t2i, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80db75d8d06c805",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# MRM\n",
    "for key in MRM_dict['model'].keys():\n",
    "    print(key)\n",
    "    print(MRM_dict['model'][key].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d23a163e69b6881",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from model.submodule.vit.vit import get_ViT\n",
    "model= get_ViT(vit_path=r'C:\\Users\\Alex\\Downloads\\deit_base_patch16_224.pth')\n",
    "# print model weight shape\n",
    "for key in model.state_dict().keys():\n",
    "    print(key)\n",
    "    print(model.state_dict()[key].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14d85b1786763420",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "# read csv file\n",
    "c=pd.read_csv('training.csv')\n",
    "def get_images_captions(caption_file):\n",
    "    captions = []\n",
    "    images= []\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                image_id, caption = parts\n",
    "                captions.append(caption)\n",
    "                images.append(str(image_id))\n",
    "            else:\n",
    "                print('Error: ', line)\n",
    "                images.append(str(image_id))\n",
    "                captions.append('')\n",
    "    return images, captions\n",
    "\n",
    "def save_to_csv(image_paths, texts, output_csv):\n",
    "    with open(output_csv, 'a+', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for img_path, text in zip(image_paths, texts):\n",
    "            if text.strip()!='':\n",
    "                writer.writerow([img_path+'.jpg', text])\n",
    "\n",
    "\n",
    "def read_txt_to_lists(file_path):\n",
    "    dict={}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # 跳过标题行\n",
    "        for row in reader:\n",
    "            if len(row) != 3:\n",
    "                print(row[0], row[1])\n",
    "            else:\n",
    "                dict[row[0]] = row[1]\n",
    "    \n",
    "    return dict\n",
    "\n",
    "path=''\n",
    "folder= ['validation']\n",
    "type= ['radiology']\n",
    "captions = []\n",
    "images= []\n",
    "dict= {}\n",
    "for f in folder:\n",
    "    for t in type:\n",
    "        lic= os.path.join(path,f,t,'licences.txt')\n",
    "        dic = read_txt_to_lists(lic)\n",
    "        dict.update(dic)\n",
    "        \n",
    "for f in folder:\n",
    "    for t in type:\n",
    "        caps= os.path.join(path,f,t,'captions.txt')\n",
    "        image, caption = get_images_captions(caps)\n",
    "        print(len(image), len(dict))\n",
    "        images.extend([dict[i] for i in image])\n",
    "        captions.extend(caption)      \n",
    "\n",
    "save_to_csv(images, captions, os.path.join(path, 'training.csv'))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae47355d73daa397",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def find_corrupted_images(image_dir):\n",
    "    corrupted_images = []\n",
    "    count=0\n",
    "    for file in os.listdir(image_dir):\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            file_path = os.path.join(image_dir, file)\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    print(count:=count+1)\n",
    "                    img = Image.open(f)\n",
    "                    img=img.convert('RGB')\n",
    "            except (OSError, IOError) as e:\n",
    "                print(f\"Error: Could not load image {file_path}. Error: {e}\")\n",
    "                corrupted_images.append(file_path)\n",
    "    return corrupted_images\n",
    "\n",
    "image_dir = './figures'  # 替换为你的图片目录路径\n",
    "corrupted_images = find_corrupted_images(image_dir)\n",
    "\n",
    "if corrupted_images:\n",
    "    print(\"Corrupted images:\")\n",
    "    for img in corrupted_images:\n",
    "        print(img)\n",
    "else:\n",
    "    print(\"No corrupted images found.\")"
   ],
   "id": "f57ff13a1b759a8e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy  as np\n",
    "che5x200 = pd.read_json('C:/Users/Alex/Downloads/chexpert_5x200.json')\n",
    "df= pd.read_csv('C:/Users/Alex/Desktop/train.csv')\n",
    "df = df.fillna(0)\n",
    "df = df[df[\"Frontal/Lateral\"] == \"Frontal\"]\n",
    "mimicmeta= pd.read_csv('C:/Users/Alex/Desktop/mimic-cxr-2.0.0-chexpert.csv')\n",
    "mimicmeta = mimicmeta.fillna(0)\n",
    "report_path= 'C:/Users/Alex/Desktop/mimic-cxr-reports'\n",
    "# Initialize the dictionary to store column names with all values as 1\n",
    "list=[ 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "task_dfs = []\n",
    "def extract_sections(report: str):\n",
    "    # This regular expression looks for the sections FINDINGS and IMPRESSION\n",
    "    # and extracts all text up to the next all-caps word or the end of the string.\n",
    "    pattern = r\"(FINDINGS:.*?)(?=\\n[A-Z]+:|$)|(IMPRESSION:.*?)(?=\\n[A-Z]+:|$)\"\n",
    "\n",
    "    extracted_text = ''\n",
    "\n",
    "    # Searching the report using the pattern\n",
    "    matches = re.findall(pattern, report, re.DOTALL)\n",
    "\n",
    "    # Each match contains tuples with the content of the sections\n",
    "    for match in matches:\n",
    "        if match[0].startswith('FINDINGS'):\n",
    "            extracted_text += match[0] + ' '\n",
    "        elif match[1].startswith('IMPRESSION'):\n",
    "            extracted_text += match[1] + ' '\n",
    "\n",
    "    return extracted_text.strip()\n",
    "# Loop through each key in che5x200\n",
    "for i, t in enumerate(list):\n",
    "    index = np.zeros(14)\n",
    "    index[i] = 1\n",
    "    df_task = df[\n",
    "        (df[\"Atelectasis\"] == index[0])\n",
    "        & (df[\"Cardiomegaly\"] == index[1])\n",
    "        & (df[\"Consolidation\"] == index[2])\n",
    "        & (df[\"Edema\"] == index[3])\n",
    "        & (df[\"Pleural Effusion\"] == index[4])]\n",
    "    mimic_task = mimicmeta[\n",
    "        (mimicmeta[\"Atelectasis\"] == index[0])\n",
    "        & (mimicmeta[\"Cardiomegaly\"] == index[1])\n",
    "        & (mimicmeta[\"Consolidation\"] == index[2])\n",
    "        & (mimicmeta[\"Edema\"] == index[3])\n",
    "        & (mimicmeta[\"Pleural Effusion\"] == index[4])]\n",
    "    print(len(df_task), len(mimic_task))\n",
    "    df_task = df_task.sample(n=200, random_state=2)\n",
    "    mimic_task = mimic_task.sample(n=600, random_state=2)\n",
    "    # get the report according to mimic_task's subject_id and study_id\n",
    "    # Initialize report content list\n",
    "    report_contents = []\n",
    "    \n",
    "    # Read the report contents\n",
    "    for j, x in mimic_task.iterrows():\n",
    "        report_file_path = os.path.join(report_path, 'files', 'p' + str(x['subject_id'])[:2], 'p' + str(int(x['subject_id'])), 's' + str(int(x['study_id'])) + '.txt')\n",
    "        if os.path.exists(report_file_path):\n",
    "            with open(report_file_path, 'r') as file:\n",
    "                txt= file.read()\n",
    "                txt = txt.replace('\\n', ' ')\n",
    "                txt = re.sub(r'\\s+', ' ', txt)\n",
    "                txt= extract_sections(txt)\n",
    "                if txt=='':\n",
    "                    continue\n",
    "                report_contents.append(txt)\n",
    "        else:\n",
    "            print(f\"Report file not found: {report_file_path}\")\n",
    "\n",
    "    # Add report contents to the DataFrame\n",
    "    df_task['report_content'] = report_contents[:200]\n",
    "    \n",
    "    df_task['Class'] = t\n",
    "    df_task = df_task[['Path', 'report_content', 'Class']]\n",
    "    df_task['Path']=df_task['Path'].replace(['CheXpert-v1.0/'], 'all_classes/')\n",
    "    task_dfs.append(df_task)\n",
    "    \n",
    "df_200 = pd.concat(task_dfs)\n",
    "df_200.to_csv('C:/Users/Alex/Desktop/df_200.csv', index=False)"
   ],
   "id": "e28bc66781df73a1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model.submodule.bert.xbert import BertLMHeadModel\n",
    "tokenizer_config='./model/submodule/bert/bert-base-uncased'\n",
    "text_decoder = BertLMHeadModel.from_pretrained(tokenizer_config)\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from model.submodule.bert.xbert import BertLMHeadModel\n",
    "\n",
    "# 配置和初始化模型和tokenizer\n",
    "tokenizer_config = './model/submodule/bert/bert-base-uncased'\n",
    "text_decoder = BertLMHeadModel.from_pretrained(tokenizer_config)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_config)\n",
    "\n",
    "# 随机生成输入数据\n",
    "batch_size = 2\n",
    "seq_length = 16\n",
    "hidden_size = text_decoder.config.hidden_size\n",
    "\n",
    "# 随机生成输入id和attention mask\n",
    "input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_length)).to(torch.int64)\n",
    "attention_mask = torch.randint(0, 2, (batch_size, seq_length)).to(torch.int64)\n",
    "\n",
    "# 随机生成encoder hidden states\n",
    "encoder_hidden_states = torch.randn(batch_size, 32, hidden_size)\n",
    "\n",
    "# 随机生成encoder attention mask\n",
    "encoder_attention_mask = torch.randint(0, 2, (batch_size, 32)).to(torch.int64)\n",
    "\n",
    "# 设置模型设备\n",
    "device = torch.device('cpu')\n",
    "text_decoder.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "encoder_hidden_states = encoder_hidden_states.to(device)\n",
    "encoder_attention_mask = encoder_attention_mask.to(device)\n",
    "\n",
    "# 前向传播测试\n",
    "outputs = text_decoder(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_attention_mask,\n",
    "    return_dict=True\n",
    ")\n"
   ],
   "id": "ceee2ee7907177be",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "path='./'\n",
    "# Function to read JSON file\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "test_data = read_json_file(os.path.join(path,'test.json'))\n",
    "\n",
    "# Extract answers from train and test data\n",
    "answers =[i for entry in test_data for i in entry['answer'].split('#')]\n",
    "\n",
    "# Convert answers to lowercase\n",
    "answers = [answer.lower() for answer in answers]\n",
    "\n",
    "# Remove duplicates\n",
    "answer_list = list(set(answers))\n",
    "\n",
    "with open(os.path.join(path,'./answer_list.json'), 'w', encoding='utf-8') as file:\n",
    "    json.dump(answer_list, file, indent=4)"
   ],
   "id": "9e5bc3bf22599032",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 模型的配置信息\n",
    "class Config:\n",
    "    vocab_size = 50257\n",
    "\n",
    "self = Config()\n",
    "\n",
    "# 生成假数据\n",
    "batch_size = 4\n",
    "sequence_length = 5\n",
    "vocab_size = self.vocab_size\n",
    "\n",
    "# 随机生成 logits\n",
    "shift_logits = torch.randn(batch_size, sequence_length, vocab_size)\n",
    "soft_labels = torch.randn(batch_size, sequence_length-1, vocab_size)\n",
    "# 随机生成标签\n",
    "shift_labels = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "\n",
    "\n",
    "# 设置 reduction 方式\n",
    "reduction = 'mean'  # 可选 'none', 'mean', 'sum'\n",
    "\n",
    "# 将 shift_logits 和 shift_labels 进行平滑\n",
    "shift_logits = shift_logits[..., :-1, :].contiguous()\n",
    "shift_labels = shift_labels[..., 1:].contiguous()\n",
    "# 生成软标签（soft labels），维度与 shift_logits 一致\n",
    "\n",
    "# # 将标签移到 logits 的设备上\n",
    "# shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "# 计算交叉熵损失\n",
    "loss_fct = CrossEntropyLoss(reduction=reduction)\n",
    "loss = loss_fct(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "loss_ =loss.view(shift_logits.size(0), -1).mean(1) if reduction == 'none' else loss\n",
    "# 如果有软标签，计算蒸馏损失\n",
    "alpha = 0.5  # 蒸馏损失的权重\n",
    "if soft_labels is not None:\n",
    "    soft_labels=F.softmax(soft_labels, dim=-1)\n",
    "    loss_distill = -torch.sum(F.log_softmax(shift_logits, dim=-1) * soft_labels, dim=-1)\n",
    "    # if reduction == 'mean':\n",
    "    #     loss_distill = loss_distill.mean()\n",
    "    # elif reduction == 'sum':\n",
    "    #     loss_distill = loss_distill.sum()\n",
    "    # 使用 shift_labels 进行掩码，排除忽略的标签\n",
    "    # loss_distill = loss_distill.view(batch_size, -1)\n",
    "    if reduction == 'none':\n",
    "        t= loss_distill * (shift_labels != -100)\n",
    "        loss_distill = t.mean(1)\n",
    "    else:\n",
    "        loss_distill = loss_distill[shift_labels != -100].mean()\n",
    "    loss = (1 - alpha) * loss_ + alpha * loss_distill \n",
    "\n",
    "# 打印损失\n",
    "print(\"Loss:\", loss)\n",
    "# torch.tensor([1],dtype=torch.float).mean().item()"
   ],
   "id": "f0bd68f0c316d523",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "result= np.load(r'C:\\Users\\Alex\\Desktop\\result.npy',allow_pickle=True)\n",
    "# result= np.load(r'C:\\Users\\Alex\\Desktop\\similarities.npy',allow_pickle=True)\n",
    "i2t=result.item().get('i2t')\n",
    "t2i=result.item().get('t2i')\n",
    "# t2i=result.squeeze()\n",
    "scores_i2t = t2i.transpose()\n",
    "def compute_precision_at_k(scores, classes, k=1):\n",
    "    precisions = [] \n",
    "\n",
    "    for i, score_row in enumerate(scores):\n",
    "        sample_class = classes[i]\n",
    "        top_k_indices = np.argsort(score_row)[-k:][::-1]  # Get top k indices\n",
    "        correct_retrievals = sum(classes[idx] == sample_class for idx in top_k_indices)\n",
    "        precision = correct_retrievals / k\n",
    "        precisions.append(precision)\n",
    "\n",
    "    return np.nanmean(precisions)\n",
    "classes=pd.read_csv(r'C:\\Users\\Alex\\Desktop\\df_200.csv')['Class']\n",
    "eval_result = {\n",
    "    \"i2t_r1\": compute_precision_at_k(scores_i2t, classes, k=1),\n",
    "    \"i2t_r2\": compute_precision_at_k(scores_i2t, classes, k=2),\n",
    "    \"i2t_r5\": compute_precision_at_k(scores_i2t, classes, k=5),\n",
    "    \"i2t_r10\": compute_precision_at_k(scores_i2t, classes, k=10),\n",
    "    \"i2t_r50\": compute_precision_at_k(scores_i2t, classes, k=50),\n",
    "}\n",
    "eval_result"
   ],
   "id": "640de7b82379f690",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "result= np.load(r'C:\\Users\\Alex\\Desktop\\result_zs.npy',allow_pickle=True)\n",
    "# result= np.load(r'C:\\Users\\Alex\\Desktop\\similarities.npy',allow_pickle=True)\n",
    "i2t=result.item().get('i2t')\n",
    "t2i=result.item().get('t2i')\n",
    "sim_matrix = i2t\n",
    "true_labels = np.repeat(np.arange(5), 200)  # 假设有序的标签\n",
    "predicted_labels = np.argmax(sim_matrix, axis=1) // 200  # 获取每行最大值的索引并转换为类别\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# 计算每类的精确度、召回率和 F1-Score\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ],
   "id": "1ee9354296b8f57f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result= np.load(r'C:\\Users\\Alex\\Desktop\\result_rt_roco.npy',allow_pickle=True)\n",
    "scores_i2t=result.item().get('i2t')\n",
    "scores_t2i=result.item().get('t2i')\n",
    "def recall_at_k(similarity_matrix, k):\n",
    "    correct = 0\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        # Get indices of the top K similar texts for image i\n",
    "        top_k_indices = np.argsort(similarity_matrix[i])[::-1][:k]\n",
    "\n",
    "        # Check if the correct text (text i) is in the top K\n",
    "        if i in top_k_indices:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(similarity_matrix)\n",
    "\n",
    "eval_result = {\n",
    "    \"i2t_r1\": recall_at_k(scores_i2t, k=1),\n",
    "    \"i2t_r5\": recall_at_k(scores_i2t, k=5),\n",
    "    \"i2t_r10\": recall_at_k(scores_i2t, k=10),\n",
    "    \"t2i_r1\": recall_at_k(scores_t2i, k=1),\n",
    "    \"t2i_r5\": recall_at_k(scores_t2i, k=5),\n",
    "    \"t2i_r10\": recall_at_k(scores_t2i, k=10),\n",
    "}"
   ],
   "id": "a0f200ba912734aa",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T15:17:25.490216Z",
     "start_time": "2024-08-06T15:17:04.668039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vis tsne\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "seed=36\n",
    "s=60\n",
    "np.random.seed(seed)  # 设置随机种子为42\n",
    "random_data = np.random.rand(1000, 256)\n",
    "data=np.load(r'C:\\Users\\Alex\\Desktop\\result_rt.npy',allow_pickle=True)\n",
    "data1=data.item().get('embeds_i')\n",
    "data1=data1.mean(axis=1)\n",
    "data2=data.item().get('embeds_t')\n",
    "# data2=data.max(axis=1)\n",
    "labels = np.array([i for i in range(5) for _ in range(200)])\n",
    "class_names = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=seed,perplexity=50,init='pca',n_iter=7000,learning_rate=2000)  # 设置随机种子为42\n",
    "tsne = umap.UMAP(n_components=2, random_state=seed, n_neighbors=13, metric='euclidean')\n",
    "data_tsne1 = tsne.fit_transform(data1)\n",
    "data_tsne2 = tsne.fit_transform(data2)\n",
    "random_tsne = tsne.fit_transform(random_data)\n",
    "\n",
    "# 可视化随机初始化数据的 t-SNE\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.scatter(random_tsne[labels == i, 0], random_tsne[labels == i, 1], label=class_names[i],s=s-10)\n",
    "plt.legend(title='Labels', loc='upper left',fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.scatter(data_tsne1[labels == i, 0], data_tsne1[labels == i, 1], label=class_names[i],s=s)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.scatter(data_tsne2[labels == i, 0], data_tsne2[labels == i, 1], label=class_names[i],s=s)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "ec3af03ce78a9448",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example 3x3 matrix\n",
    "matrix = np.array([\n",
    "    [0.78, 0.30, 0.31],\n",
    "    [0.29, 0.85, 0.44],\n",
    "    [0.34, 0.53, 0.96]\n",
    "]).transpose()\n",
    "# normalize the matrix on dimension 1\n",
    "matrix = matrix / matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "heatmap = plt.imshow(matrix, cmap='Blues', interpolation='nearest')\n",
    "\n",
    "# Annotate each cell with the numeric value in black\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        plt.text(j, i, f\"{matrix[i, j]:.2f}\", ha='center', va='center', color='black', fontsize=24)\n",
    "\n",
    "# Hide the axes\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "ade31666de8602d7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7f752ae47e114452",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
